{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NO17wlbpBaqn"
   },
   "source": [
    "# Data import\n",
    "## Question 0 - Get common wikidata occupations\n",
    "\n",
    "> Write a sparql query that retrieves the top 20 occupations on wikidata (wikidata property P106).\n",
    "\n",
    "You may use the interface https://query.wikidata.org/ to try different queries. Here are some example sparql queries: https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service/queries/examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zbrmD_sOBitt"
   },
   "outputs": [],
   "source": [
    "query = \"\"\"  \n",
    " SELECT ?o\n",
    " WHERE {\n",
    "     ?item wdt:P106 ?o\n",
    " }\n",
    " GROUP BY ?o\n",
    " ORDER BY DESC(COUNT(?o))\n",
    " LIMIT 20\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d5Noin8EBwgT"
   },
   "source": [
    "The following assertion should pass if your answer is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lKDALPf5Bveq"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "occupations = ['Q82955', 'Q1650915', 'Q937857', 'Q36180', 'Q33999', 'Q1028181', 'Q1930187', 'Q1622272', 'Q177220', 'Q49757', 'Q36834', 'Q47064', 'Q40348', 'Q10800557', 'Q43845', 'Q201788', 'Q639669', 'Q2526255', 'Q28389', 'Q39631']\n",
    "\n",
    "def evalSparql(query):\n",
    "    return requests.post('https://query.wikidata.org/sparql', data=query, headers={\n",
    "        'content-type': 'application/sparql-query',\n",
    "        'accept': 'application/json',\n",
    "        'user-agent': 'User:Tpt'\n",
    "    }).json()['results']['bindings']\n",
    "\n",
    "myOccupations = [val['o']['value'].replace('http://www.wikidata.org/entity/', '') for val in evalSparql(query)]\n",
    "assert(frozenset(occupations) == frozenset(myOccupations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KvJOjnjJB9N1"
   },
   "source": [
    "## Occupations labels\n",
    "\n",
    "We load the labels of the occupations from Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EK3ptLUZB9wm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Q82955': 'politician', 'Q177220': 'singer', 'Q201788': 'historian', 'Q639669': 'musician', 'Q937857': 'association football player', 'Q1028181': 'painter', 'Q1622272': 'university teacher', 'Q1650915': 'researcher', 'Q1930187': 'journalist', 'Q2526255': 'film director', 'Q10800557': 'film actor', 'Q40348': 'lawyer', 'Q33999': 'actor', 'Q36834': 'composer', 'Q36180': 'writer', 'Q43845': 'businessperson', 'Q39631': 'physician', 'Q28389': 'screenwriter', 'Q49757': 'poet', 'Q47064': 'military personnel'}\n"
     ]
    }
   ],
   "source": [
    "occupations_label = {}\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT ?o ?oLabel \n",
    "WHERE { \n",
    "    VALUES ?o { %s } \n",
    "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "}\"\"\"% ' '.join('wd:' + o for o in occupations)\n",
    "\n",
    "for result in evalSparql(query):\n",
    "    occupations_label[result['o']['value'].replace('http://www.wikidata.org/entity/', '')] = result['oLabel']['value']\n",
    "\n",
    "print(occupations_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Up5cRHQQD5Fg"
   },
   "source": [
    "We load *all* the labels of the occupations from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zw0CMXCFD69B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Q82955': ['politician', 'political leader', 'political figure', 'polit.'], 'Q177220': ['singer', 'singer', 'vocalist'], 'Q201788': ['historian', 'historians', 'historiographer'], 'Q639669': ['musician'], 'Q937857': ['association football player', 'footballer', 'football player', 'soccer player', 'association footballer'], 'Q1028181': ['painter'], 'Q1622272': ['university teacher', 'professor', 'lecturer', 'college lecturer', 'college professor', 'university teachers'], 'Q1650915': ['researcher', 'researchers', 'research personnel'], 'Q1930187': ['journalist', 'journo'], 'Q2526255': ['film director', 'director', 'movie director', 'motion picture director'], 'Q10800557': ['film actor', 'film actress', 'film actor', 'movie actor', 'movie actress'], 'Q40348': ['lawyer', 'attorney', 'Jurisprudente', 'lawyers'], 'Q33999': ['actor', 'actress', 'actors', 'actresses', 'thespian'], 'Q36834': ['composer'], 'Q36180': ['writer', 'author', 'authors', 'writers'], 'Q43845': ['businessperson', 'businessman', 'dealer', 'business person', 'business woman', 'businesswoman', 'business man'], 'Q39631': ['physician', 'doctor', 'medical doctor', 'medical practitioner', 'physicians'], 'Q28389': ['screenwriter', 'writer', 'screen writer', 'scriptwriter', 'scenarist', 'script writer'], 'Q49757': ['poet', 'bard', 'poetess'], 'Q47064': ['military personnel', 'military member']}\n"
     ]
    }
   ],
   "source": [
    "occupations_labels = {k: [v] for k, v in occupations_label.items()}\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT ?o ?altLabel \n",
    "WHERE {\n",
    "  VALUES ?o { %s }\n",
    "  ?o skos:altLabel ?altLabel . FILTER (lang(?altLabel) = \"en\")\n",
    "}\"\"\" % ' '.join('wd:' + o for o in occupations) \n",
    "\n",
    "for result in evalSparql(query):\n",
    "    occupations_labels[result['o']['value'].replace('http://www.wikidata.org/entity/', '')].append(result['altLabel']['value'])\n",
    "\n",
    "print(occupations_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "30_FP43GCET0"
   },
   "source": [
    "## Wikipedia articles\n",
    "\n",
    "Here we load the training and the testing sets. To save memory space we use a generator that will read the file each time we iterate over the training or the testing examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ii1YlSQwCE2h"
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "def loadJson(filename):\n",
    "    with gzip.open(filename, 'rt') as fp:\n",
    "        for line in fp:\n",
    "            yield json.loads(line)\n",
    "\n",
    "class MakeIter(object):\n",
    "    def __init__(self, generator_func, **kwargs):\n",
    "        self.generator_func = generator_func\n",
    "        self.kwargs = kwargs\n",
    "    def __iter__(self):\n",
    "        return self.generator_func(**self.kwargs)\n",
    "\n",
    "training_set = MakeIter(loadJson, filename='wiki-train.json.gz')\n",
    "testing_set = MakeIter(loadJson, filename='wiki-test.json.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nd1E_n3t5GLt"
   },
   "source": [
    "# Extract occupations from summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RzLy5Eo65LT2"
   },
   "source": [
    "## Task 1 - Dictionnary extraction\n",
    "\n",
    "> Using ```occupations_labels``` dictionnary, identify all occupations for each articles. Complete the function predict_dictionary() below and then evaluate the accuracy of such approach. It will serve as a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UqvWsFeb5Gpp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5678982587144555"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_dictionnary(example, occupations_labels):\n",
    "    ## example['summary'] contains the summary of the article\n",
    "    ## Code here\n",
    "    result = []\n",
    "    for k in occupations_labels:\n",
    "        v = occupations_labels[k]\n",
    "        for o in v:\n",
    "            if o in example['summary']:\n",
    "                result.append(k)\n",
    "    return result\n",
    "    \n",
    "def evaluate_dictionnary(training_set, occupations_labels):\n",
    "    nexample = 0\n",
    "    accuracy = 0.\n",
    "    prediction = None\n",
    "    for example in training_set:\n",
    "        prediction = predict_dictionnary(example, occupations_labels)\n",
    "        \n",
    "        p = frozenset(prediction)\n",
    "        g = frozenset(example['occupations'])\n",
    "        accuracy += 1.*len(p & g) / len(p | g)\n",
    "        nexample += 1\n",
    "    return accuracy / nexample\n",
    "\n",
    "evaluate_dictionnary(training_set, occupations_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NuBXTLtf6C9r"
   },
   "source": [
    "## Task 2 - Simple neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cKz_y84x6D35"
   },
   "source": [
    "We load the articles \"summary\" and we encode the data using the average of the word vectors.\n",
    "This is done with spacy loaded with the fast text vectors.\n",
    "To do the installation/loading [takes 8-10 minutes, dl 1.2Go] :\n",
    "```\n",
    "pip3 install spacy\n",
    "\n",
    "wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
    "```\n",
    "Use the following command to create the new model from the fast text vectors (please adjust the path_to_file)\n",
    "```\n",
    "python3 -m spacy init-model en path_to_file/en_vectors_wiki_lg --vectors-loc cc.en.300.vec.gz\n",
    "rm cc.en.300.vec.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hGzaTLk26HU9"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_vectors_wiki_lg')\n",
    "\n",
    "def vectorize(dataset, nlp):\n",
    "    result = {}\n",
    "    for example in dataset:\n",
    "        doc = nlp(example['summary'], disable=['parser', 'tagger'])\n",
    "        result[example['title']] = {}\n",
    "        result[example['title']]['vector'] = doc.vector\n",
    "        if 'occupations' in example:\n",
    "            result[example['title']]['occupations'] = example['occupations']\n",
    "    return result\n",
    "\n",
    "vectorized_training = vectorize(training_set, nlp)\n",
    "vectorized_testing = vectorize(testing_set, nlp)\n",
    "nlp = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we encode the output as multi-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xfndy8sP6RTg"
   },
   "outputs": [],
   "source": [
    "# We encode the data\n",
    "import numpy as np\n",
    "\n",
    "inputs = np.array([vectorized_training[article]['vector'] for article in vectorized_training])\n",
    "outputs = np.array([[(1 if occupation in vectorized_training[article]['occupations'] else 0)\n",
    "                    for occupation in occupations ] for article in vectorized_training])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "print(len(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ss1hJOSM8GQC"
   },
   "source": [
    "> Using keras, define a sequential neural network with two layers. Use **binary_crossentropy** as a loss function and **sigmoid** as the activation function of the output layer\n",
    "\n",
    "You can look into the documentation here: https://keras.io/getting-started/sequential-model-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Hmyhmzn6Vsc"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "keras.layers.Dense(300, input_shape=inputs[0].shape),\n",
    "keras.layers.Activation('relu'),\n",
    "keras.layers.Dense(20),\n",
    "keras.layers.Activation('sigmoid'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BLlAUDXC6e4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 271049 samples\n",
      "271049/271049 [==============================] - 17s 63us/sample - loss: 0.0870 - accuracy: 0.9691\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x221b7cdd0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Then train the model on ```inputs``` and ```outputs```\n",
    "model.fit(inputs,outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lTR5RvaN6mlJ"
   },
   "source": [
    "> Complete the function predict: output the list of occupations where the corresponding neuron on the output layer of our model has a value > 0.1\n",
    "For predicting with your model use **model.predict_on_batch()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0NuHZcRT6nJM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Q33999', 'Q177220', 'Q36834', 'Q10800557', 'Q639669']\n"
     ]
    }
   ],
   "source": [
    "def predict(model, article_name, vectorized_dataset):\n",
    "    prediction = None\n",
    "    ## Code here\n",
    "    prediction = model.predict_on_batch(vectorized_dataset[article_name]['vector'].reshape(1,300))\n",
    "    result = []\n",
    "    index = np.where(prediction > 0.1)\n",
    "    for i in index[1]:\n",
    "        result.append(occupations[i])\n",
    "    return result\n",
    "\n",
    "print(predict(model, 'Elvis_Presley', vectorized_training))\n",
    "# should be subset of {'Q33999', 'Q177220', 'Q10800557', 'Q28389'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aJDYp3f06toj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6541374477249777"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_nn(vectorized_training, model):\n",
    "    nexample = 0\n",
    "    accuracy = 0.\n",
    "    prediction = None\n",
    "    for article_name in vectorized_training:\n",
    "        prediction = predict(model, article_name, vectorized_training)\n",
    "        p = frozenset(prediction)\n",
    "        g = frozenset(vectorized_training[article_name]['occupations'])\n",
    "        accuracy += 1.*len(p & g) / len(p | g)\n",
    "        nexample += 1\n",
    "    return accuracy / nexample\n",
    "evaluate_nn(vectorized_training, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8sabZFs68cH7"
   },
   "source": [
    "## Task 3 - Your approach\n",
    "\n",
    "> Propose your own approach (extend previous examples or use original approaches) to improve the accuracy for this task. Apply it to the testing set and put the result as a json file with your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OYg-kgV88cnr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 271049 samples\n",
      "Epoch 1/20\n",
      "271049/271049 [==============================] - 4s 14us/sample - loss: 0.1210 - accuracy: 0.9599\n",
      "Epoch 2/20\n",
      "271049/271049 [==============================] - 3s 12us/sample - loss: 0.0787 - accuracy: 0.9712\n",
      "Epoch 3/20\n",
      "271049/271049 [==============================] - 3s 12us/sample - loss: 0.0735 - accuracy: 0.9732\n",
      "Epoch 4/20\n",
      "271049/271049 [==============================] - 3s 12us/sample - loss: 0.0712 - accuracy: 0.9742\n",
      "Epoch 5/20\n",
      "271049/271049 [==============================] - 3s 12us/sample - loss: 0.0698 - accuracy: 0.9747\n",
      "Epoch 6/20\n",
      "271049/271049 [==============================] - 3s 12us/sample - loss: 0.0688 - accuracy: 0.9751\n",
      "Epoch 7/20\n",
      "271049/271049 [==============================] - 3s 12us/sample - loss: 0.0681 - accuracy: 0.9754\n",
      "Epoch 8/20\n",
      "271049/271049 [==============================] - 4s 13us/sample - loss: 0.0675 - accuracy: 0.9756\n",
      "Epoch 9/20\n",
      "271049/271049 [==============================] - 4s 13us/sample - loss: 0.0670 - accuracy: 0.9758\n",
      "Epoch 10/20\n",
      "271049/271049 [==============================] - 3s 12us/sample - loss: 0.0666 - accuracy: 0.9759\n",
      "Epoch 11/20\n",
      "271049/271049 [==============================] - 4s 13us/sample - loss: 0.0662 - accuracy: 0.9761\n",
      "Epoch 12/20\n",
      "271049/271049 [==============================] - 4s 14us/sample - loss: 0.0659 - accuracy: 0.9762\n",
      "Epoch 13/20\n",
      "271049/271049 [==============================] - 3s 13us/sample - loss: 0.0655 - accuracy: 0.9763\n",
      "Epoch 14/20\n",
      "271049/271049 [==============================] - 4s 14us/sample - loss: 0.0652 - accuracy: 0.9765\n",
      "Epoch 15/20\n",
      "271049/271049 [==============================] - 3s 13us/sample - loss: 0.0649 - accuracy: 0.9765\n",
      "Epoch 16/20\n",
      "271049/271049 [==============================] - 3s 13us/sample - loss: 0.0647 - accuracy: 0.9767\n",
      "Epoch 17/20\n",
      "271049/271049 [==============================] - 3s 12us/sample - loss: 0.0644 - accuracy: 0.9768\n",
      "Epoch 18/20\n",
      "271049/271049 [==============================] - 3s 12us/sample - loss: 0.0642 - accuracy: 0.9768\n",
      "Epoch 19/20\n",
      "271049/271049 [==============================] - 3s 12us/sample - loss: 0.0640 - accuracy: 0.9770\n",
      "Epoch 20/20\n",
      "271049/271049 [==============================] - 3s 12us/sample - loss: 0.0637 - accuracy: 0.9770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x215e3fcd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "keras.layers.Dense(300, input_shape=inputs[0].shape),\n",
    "keras.layers.Activation('relu'),\n",
    "keras.layers.Dense(20),\n",
    "keras.layers.Activation('sigmoid'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(inputs, outputs, epochs=20, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6837642196526171"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_nn(vectorized_training, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wnQBS0Qp8gbk"
   },
   "source": [
    "***IMPORTANT*** Output format of requested file 'results.json.gz': each line must be a json string representing a dictionnary:\n",
    "> ```{ 'title': THE_ARTICLE_NAME, 'prediction': [THE_LIST_OF_OCCUPATIONS]}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z9GIvM8T8ije"
   },
   "outputs": [],
   "source": [
    "# For example if testset_solutions is a dictionnary: article_name (key) -> prediction_list (value) use this function:\n",
    "def export(testset_solutions):\n",
    "    with gzip.open('results.json.gz', 'wt') as output:\n",
    "        for article in testset_solutions:\n",
    "            output.write(json.dumps({'title':article, 'prediction':testset_solutions[article]}) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o8Aa5HaP8l6l"
   },
   "outputs": [],
   "source": [
    "testset_solutions = {}\n",
    "for article in vectorized_testing:\n",
    "    testset_solutions[article] = predict(model, article, vectorized_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "export(testset_solutions)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lab4_DeepLearning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
